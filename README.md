**Abstract**

Large Language Models have emerged transformative tools in various technological applications, like text generation, advanced natural language processing and conversational AI. 
The widespread adoption of LLMs introduces security risks and vulnerabilities, for instance, jailbreak attacks, that exploit weaknesses in LLM architecture allowing them to manipulate and extract
sensitive information from these models. Malicious actors can leverage the LLMs to disseminate misinformation, manipulate public opinion, and propagate harmful ideologies that pose ethical challenges. 
Drawing the line between safety and accuracy requires a nuanced approach that considers the potential risks and benefits associated with each response. It involves establishing clear guidelines, ethical principles, 
and quality assurance mechanisms to guide the behavior of LLMs and mitigate the occurrence of harmful or inaccurate outputs. Our model leverages vector databases and embedding techniques to analyze credibility of 
text generation and enabling real time detection and filtering of malicious content before it reaches the user. Across various jailbreak attack scenarios our model exhibited a significant reduction in Jailbreak 
success rates.  

Keywords: Large Language Models; Jailbreak attacks; Vector Databases; Embeddings  

[To interact with the chat model-](Response_Generation.ipynb)
